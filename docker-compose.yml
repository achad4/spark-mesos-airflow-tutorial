version: "3.0"
networks:
  airflow:
    driver: bridge
volumes: { pg_data: {}, redis: {} }
services:
  airflow_server: &airflow_base
    image: airflow #docker.sailthru.com/ds/airflow:1.10.0-spark2.2.1
    networks: ["airflow"]
    restart: always
    hostname: airflow-server
    container_name: airflow_server
    depends_on:
        - postgresql
        - redis
    # env_file: ./config-dev.env
    volumes:
        - ./:/usr/local/airflow
    ports:
        - "8080:8080"
        - "8793:8793"
        - "5555:5555"
    entrypoint: ["/usr/local/airflow/script/entrypoint.sh", "webserver"]
    healthcheck:
        test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
        interval: 30s
        timeout: 30s
        retries: 3
  airflow_scheduler:
    <<: *airflow_base
    hostname: airflow-scheduler
    container_name: airflow_scheduler
    entrypoint: ["/usr/local/airflow/script/entrypoint.sh", "scheduler"]
    depends_on:
        - airflow_server
    ports:
      - "8081:8081"
  airflow_worker:
    <<: *airflow_base
    container_name: airflow_worker
    hostname: airflow-worker
    entrypoint: ["/usr/local/airflow/script/entrypoint.sh", "worker"]
    depends_on:
        - airflow_server
    # env_file: ./config-dev.env
    environment:
      C_FORCE_ROOT: "True"
    ports:
        - "8082:8082"


  mesos-master:
    image: mesosphere/mesos-master:1.0.1-2.0.93.ubuntu1404
    container_name: mesos-master
    hostname: mesos-master
    networks: ["airflow"]
    environment:
      MESOS_ZK: zk://zk:2181/mesos
      MESOS_QUORUM: 1
      MESOS_CLUSTER: docker-compose
      MESOS_REGISTRY: replicated_log # default is in_memory for some reason
      MESOS_HOSTNAME: mesos-master
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    depends_on:
      - zookeeper
    ports:
      - "5050:5050"
  mesos-slave:
    image: mesosphere/mesos-slave:1.0.1-2.0.93.ubuntu1404 #mesos-slave:1.0.1-2.0.93-hadoop #
    container_name: mesos-slave
    privileged: true
    hostname: mesos-slave
    networks: ["airflow"]
    environment:
      MESOS_MASTER: zk://zk:2181/mesos
      MESOS_CONTAINERIZERS: docker
      MESOS_PORT: 5051
      MESOS_RESOURCES: ports(*):[11000-11999]
      MESOS_HOSTNAME: localhost
      MESOS_WORK_DIR: /tmp/mesos
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    working_dir: /usr/local/
    volumes:
      - /tmp/mesos:/tmp/mesos
      - ./python/:/python/
      - ./airflow/:/usr/local/airflow
      - ~/docker.tar.gz:/etc/docker.tar.gz
      - /usr/local/bin/docker:/usr/bin/docker
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - mesos-master
      - zookeeper
    entrypoint: mesos-slave
    ports:
      - "5051:5051"
  spark-mesos:
    networks: ["airflow"]
    hostname: spark-mesos
    image: tutorial
    container_name: spark-mesos
    hostname: spark-mesos
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      SPARK_MESOS_DISPATCHER_HOST: spark-mesos
    depends_on:
      - zookeeper
    entrypoint: /usr/local/spark/bin/spark-class org.apache.spark.deploy.mesos.MesosClusterDispatcher --master mesos://zk://zookeeper:2181/mesos --zk zk:2181 --name spark --port 7077 --host spark-mesos --webui-port 8084
    ports:
      - "8084:8084"
      - "7077:7077"
  zookeeper:
    container_name: zookeeper
    hostname: zookeeper
    image: wurstmeister/zookeeper
    networks:
      airflow:
        aliases: [ "zk" ]
    ports: [ "2181:2181" ]
  redis:
    container_name: redis
    hostname: redis
    networks: [ "airflow" ]
    image: redis:2.8.9
    volumes: [ "redis:/data" ]
    ports: [ "6379:6379" ]
  postgresql:
    container_name: postgresql
    hostname: postgresql
    networks: [ "airflow" ]
    image: postgres:9.5
    volumes:
      - pg_data:/var/lib/postgresql/data
    ports: [ "5432:5432" ]
    
